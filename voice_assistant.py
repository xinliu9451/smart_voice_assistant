 #!/usr/bin/env python3
"""
è¯­éŸ³æ™ºèƒ½åŠ©æ‰‹ - å‚»å¦
åŠŸèƒ½æµç¨‹ï¼š
1. å…³é”®è¯å”¤é†’ï¼ˆ"ä½ å¥½å‚»å¦" æˆ– "å‚»å¦"ï¼‰
2. TTSæ’­æŠ¥"ä¸»äººï¼Œæœ‰ä»€ä¹ˆå©å’ï¼Ÿ"
3. è¯­éŸ³è¯†åˆ«ï¼ˆå¸¦VADï¼Œ3ç§’é™éŸ³åç»“æŸï¼‰
4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
5. TTSæ’­æŠ¥å›å¤
6. è¿”å›å…³é”®è¯æ£€æµ‹ï¼Œç­‰å¾…ä¸‹æ¬¡å”¤é†’
"""

import os
import sys
import time
import numpy as np
import requests
import soundfile as sf

try:
    import sounddevice as sd
except ImportError:
    print("è¯·å…ˆå®‰è£… sounddevice: pip install sounddevice")
    sys.exit(-1)

import sherpa_onnx

# ==================== é…ç½®å‚æ•° ====================
# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# æ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹äºè„šæœ¬ç›®å½•ï¼‰
KWS_TOKENS = os.path.join(SCRIPT_DIR, "kws", "kws", "tokens.txt")
KWS_ENCODER = os.path.join(SCRIPT_DIR, "kws", "kws", "encoder.int8.onnx")
KWS_DECODER = os.path.join(SCRIPT_DIR, "kws", "kws", "decoder.onnx")
KWS_JOINER = os.path.join(SCRIPT_DIR, "kws", "kws", "joiner.int8.onnx")
KWS_KEYWORDS_FILE = os.path.join(SCRIPT_DIR, "kws", "kws", "test_wavs", "test_keywords.txt")

ASR_MODEL = os.path.join(SCRIPT_DIR, "StreamingAsr", "model", "model.int8.onnx")
ASR_TOKENS = os.path.join(SCRIPT_DIR, "StreamingAsr", "model", "tokens.txt")
VAD_MODEL = os.path.join(SCRIPT_DIR, "StreamingAsr", "model", "vad.onnx")

TTS_MODEL_DIR = os.path.join(SCRIPT_DIR, "TTS", "matcha-icefall-zh-baker")

# Ollamaé…ç½®
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen3:4b-instruct-2507-q4_K_M"

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯å‚»å¦ï¼Œä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚ä½ çš„ç‰¹ç‚¹ï¼š
1. ä½ å«å‚»å¦ï¼Œæ˜¯ä¸»äººçš„è´´å¿ƒåŠ©æ‰‹
2. ä½ èªæ˜ã€æ´»æ³¼ã€å‹å–„ï¼Œå–œæ¬¢å¸®åŠ©ä¸»äºº
3. å›ç­”ä¸è¦å¤ªé•¿ï¼Œé€šå¸¸æ§åˆ¶åœ¨10-100å­—ä¹‹é—´ï¼Œå› ä¸ºè¦è¯­éŸ³æ’­æŠ¥
4. è¯­æ°”äº²åˆ‡è‡ªç„¶ï¼Œå¯ä»¥ç§°å‘¼å¯¹æ–¹ä¸º"ä¸»äºº"
5. å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯šå®è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯"""

# éŸ³é¢‘å‚æ•°
SAMPLE_RATE = 16000
VAD_SILENCE_DURATION = 2.0  # é™éŸ³æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰

# ====================================================

# å…¨å±€å˜é‡
killed = False
 
# RAG ç›¸å…³ï¼ˆå¯é€‰ï¼‰
# å¦‚æœä¾èµ–æœªå®‰è£…æˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è‡ªåŠ¨é™çº§ä¸ºæ™®é€šå¯¹è¯
RAG_ENABLED = False
rag_retriever = None
try:
    # åŠ å…¥ RAG æ¨¡å—è·¯å¾„å¹¶å°è¯•åŠ è½½æ£€ç´¢å™¨
    sys.path.append(os.path.join(SCRIPT_DIR, "RAG"))
    from vector import retriever as rag_retriever  # type: ignore
    RAG_ENABLED = True
except Exception as e:
    print(f"RAG æœªå¯ç”¨ï¼ˆå¯é€‰ï¼‰ï¼š{e}")


class VoiceAssistant:
    """è¯­éŸ³åŠ©æ‰‹ä¸»ç±»"""
    
    def __init__(self):
        self.kws = None
        self.asr = None
        self.vad = None
        self.tts = None
        self.conversation_history = []
    
    def augment_with_rag(self, question: str) -> str:
        """ä½¿ç”¨æœ¬åœ° RAG çŸ¥è¯†åº“ä¸ºé—®é¢˜æ£€ç´¢ç›¸å…³èµ„æ–™ï¼Œå¹¶è¿”å›æ‹¼æ¥åçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ã€‚
        ä¾èµ– RAG/ ä¸‹çš„å‘é‡åº“ï¼Œç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨æ„å»ºã€‚
        """
        if not RAG_ENABLED or rag_retriever is None:
            return ""
        try:
            docs = rag_retriever.invoke(question)
            if not docs:
                return ""
            # å°†æ£€ç´¢åˆ°çš„å†…å®¹ç®€æ´æ‹¼æ¥ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
            contents = []
            for d in docs:
                content = getattr(d, "page_content", None)
                if content:
                    contents.append(str(content))
            reviews_text = "\n".join(contents[:5])
            return reviews_text
        except Exception as e:
            print(f"RAG è°ƒç”¨å¤±è´¥ï¼š{e}")
            return ""
    
    def check_files(self):
        """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        files_to_check = {
            "KWS tokens": KWS_TOKENS,
            "KWS encoder": KWS_ENCODER,
            "KWS decoder": KWS_DECODER,
            "KWS joiner": KWS_JOINER,
            "KWS keywords": KWS_KEYWORDS_FILE,
            "ASR model": ASR_MODEL,
            "ASR tokens": ASR_TOKENS,
            "VAD model": VAD_MODEL,
            "TTS acoustic": os.path.join(TTS_MODEL_DIR, "model-steps-3.onnx"),
            "TTS vocoder": os.path.join(TTS_MODEL_DIR, "vocos-22khz-univ.onnx"),
        }
        
        missing = []
        for name, path in files_to_check.items():
            if not os.path.exists(path):
                missing.append(f"  âŒ {name}: {path}")
        
        if missing:
            print("ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
            for m in missing:
                print(m)
            return False
        
        print("âœ“ æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
        return True
        
    def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 50)
        print("æ­£åœ¨åˆå§‹åŒ–å‚»å¦è¯­éŸ³åŠ©æ‰‹...")
        print("=" * 50)
        
        if not self.check_files():
            sys.exit(-1)
        
        # åˆå§‹åŒ–å…³é”®è¯æ£€æµ‹
        print("1. åŠ è½½å…³é”®è¯å”¤é†’æ¨¡å‹...")
        self.kws = sherpa_onnx.KeywordSpotter(
            tokens=KWS_TOKENS,
            encoder=KWS_ENCODER,
            decoder=KWS_DECODER,
            joiner=KWS_JOINER,
            num_threads=2,
            keywords_file=KWS_KEYWORDS_FILE,
            keywords_score=2.0,
            keywords_threshold=0.1,
            max_active_paths=4,
            num_trailing_blanks=1,
            provider="cpu",
        )
        print("   âœ“ å…³é”®è¯æ£€æµ‹æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ï¼ˆä¸¥æ ¼æŒ‰ StreamingAsr/infer.py çš„æ–¹å¼ï¼‰
        print("2. åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
        self.asr = sherpa_onnx.OfflineRecognizer.from_sense_voice(
            model=ASR_MODEL,
            tokens=ASR_TOKENS,
            num_threads=2,
            use_itn=True,  # ä¸ infer.py ä¸€è‡´
            language="zh",
            debug=False,
            hr_dict_dir="",   # ä¸ infer.py é»˜è®¤ä¸€è‡´
            hr_rule_fsts="",
            hr_lexicon="",
        )
        print("   âœ“ è¯­éŸ³è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–VAD
        print("3. åŠ è½½VADæ¨¡å‹...")
        vad_config = sherpa_onnx.VadModelConfig()
        vad_config.silero_vad.model = VAD_MODEL
        vad_config.silero_vad.threshold = 0.5
        vad_config.silero_vad.min_silence_duration = VAD_SILENCE_DURATION
        vad_config.silero_vad.min_speech_duration = 0.25
        vad_config.silero_vad.max_speech_duration = 15
        vad_config.sample_rate = SAMPLE_RATE
        self.vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=100)
        print("   âœ“ VADæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–TTS
        print("4. åŠ è½½TTSæ¨¡å‹...")
        acoustic_model = os.path.join(TTS_MODEL_DIR, "model-steps-3.onnx")
        vocoder = os.path.join(TTS_MODEL_DIR, "vocos-22khz-univ.onnx")
        lexicon = os.path.join(TTS_MODEL_DIR, "lexicon.txt")
        tokens = os.path.join(TTS_MODEL_DIR, "tokens.txt")
        dict_dir = os.path.join(TTS_MODEL_DIR, "dict")
        rule_fsts = f"{os.path.join(TTS_MODEL_DIR, 'phone.fst')},{os.path.join(TTS_MODEL_DIR, 'date.fst')},{os.path.join(TTS_MODEL_DIR, 'number.fst')}"
        
        tts_config = sherpa_onnx.OfflineTtsConfig(
            model=sherpa_onnx.OfflineTtsModelConfig(
                matcha=sherpa_onnx.OfflineTtsMatchaModelConfig(
                    acoustic_model=acoustic_model,
                    vocoder=vocoder,
                    lexicon=lexicon,
                    tokens=tokens,
                    dict_dir=dict_dir,
                ),
                provider="cpu",
                debug=0,
                num_threads=2,
            ),
            rule_fsts=rule_fsts,
            max_num_sentences=1,
        )
        self.tts = sherpa_onnx.OfflineTts(tts_config)
        print("   âœ“ TTSæ¨¡å‹åŠ è½½å®Œæˆ")
        
        print("=" * 50)
        print("å‚»å¦å·²å°±ç»ªï¼Œç­‰å¾…å”¤é†’...")
        print("å”¤é†’è¯ï¼š'ä½ å¥½å‚»å¦' æˆ– 'å‚»å¦'")
        print("=" * 50 + "\n")
    
    def play_audio(self, audio_samples, sample_rate):
        """æ’­æ”¾éŸ³é¢‘"""
        sd.play(audio_samples, samplerate=sample_rate)
        sd.wait()
    
    def tts_speak(self, text):
        """è¯­éŸ³æ’­æŠ¥"""
        print(f"ğŸ”Š å‚»å¦: {text}")
        audio = self.tts.generate(text, speed=1.0)
        if len(audio.samples) > 0:
            self.play_audio(audio.samples, audio.sample_rate)
    
    def wait_for_keyword(self):
        """ç­‰å¾…å…³é”®è¯å”¤é†’"""
        print("ğŸ§ æ­£åœ¨ç›‘å¬å…³é”®è¯...")
        stream = self.kws.create_stream()
        samples_per_read = int(0.1 * SAMPLE_RATE)
        
        with sd.InputStream(channels=1, dtype="float32", samplerate=SAMPLE_RATE) as audio_stream:
            while not killed:
                samples, _ = audio_stream.read(samples_per_read)
                samples = samples.reshape(-1)
                stream.accept_waveform(SAMPLE_RATE, samples)
                
                while self.kws.is_ready(stream):
                    self.kws.decode_stream(stream)
                    result = self.kws.get_result(stream)
                    if result:
                        keyword = result.strip()
                        if "å‚»å¦" in keyword:
                            print(f"âœ¨ æ£€æµ‹åˆ°å”¤é†’è¯: {keyword}")
                            self.kws.reset_stream(stream)
                            return True
                        self.kws.reset_stream(stream)
        return False
    
    def listen_and_recognize(self):
        """ç›‘å¬å¹¶è¯†åˆ«è¯­éŸ³ï¼ˆå¸¦VADï¼‰"""
        print("ğŸ‘‚ è¯·è¯´è¯...")
        
        buffer = []
        offset = 0
        window_size = self.vad.config.silero_vad.window_size
        samples_per_read = int(0.1 * SAMPLE_RATE)
        
        started = False
        started_time = None
        
        with sd.InputStream(channels=1, dtype="float32", samplerate=SAMPLE_RATE) as audio_stream:
            while not killed:
                samples, _ = audio_stream.read(samples_per_read)
                samples = samples.reshape(-1)
                
                buffer = np.concatenate([buffer, samples])
                
                # VADå¤„ç†
                while offset + window_size < len(buffer):
                    self.vad.accept_waveform(buffer[offset : offset + window_size])
                    if not started and self.vad.is_speech_detected():
                        started = True
                        started_time = time.time()
                        print("   ğŸ¤ å¼€å§‹è¯´è¯...")
                    offset += window_size
                
                # æ¸…ç†ç¼“å†²åŒº
                if not started:
                    if len(buffer) > 10 * window_size:
                        offset -= len(buffer) - 10 * window_size
                        buffer = buffer[-10 * window_size :]
                
                # å®æ—¶è¯†åˆ«ï¼ˆä¸æ‰“å°ï¼Œåªç”¨äºå†…éƒ¨å¤„ç†ï¼‰
                if started and time.time() - started_time > 0.2:
                    stream = self.asr.create_stream()
                    stream.accept_waveform(SAMPLE_RATE, buffer)
                    self.asr.decode_stream(stream)
                    started_time = time.time()
                
                # å¤„ç†VADæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µï¼ˆæ£€æµ‹åˆ°é™éŸ³ç»“æŸï¼‰
                while not self.vad.empty():
                    stream = self.asr.create_stream()
                    stream.accept_waveform(SAMPLE_RATE, self.vad.front.samples)
                    self.vad.pop()
                    self.asr.decode_stream(stream)
                    text = stream.result.text.strip()
                    
                    buffer = []
                    offset = 0
                    started = False
                    started_time = None
                    
                    if text:
                        print(f"   âœ“ è¯†åˆ«å®Œæˆ: {text}")
                        return text
        
        return ""

    def listen_and_recognize_with_timeout(self, idle_timeout_seconds: float):
        """åœ¨ç»™å®šçš„ç©ºé—²è¶…æ—¶æ—¶é—´å†…ç­‰å¾…å¹¶è¯†åˆ«ä¸€æ®µè¯ã€‚
        è¿”å› (text, timed_out)ã€‚
        - text: è¯†åˆ«åˆ°çš„æ–‡æœ¬ï¼›å¦‚æœè¶…æ—¶æˆ–æ— å†…å®¹åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
        - timed_out: å¦‚æœåœ¨ idle_timeout_seconds å†…æ²¡æœ‰å¼€å§‹è¯´è¯åˆ™ä¸º True
        """
        print(f"â³ ç­‰å¾…è¯´è¯ï¼ˆæœ€å¤š{int(idle_timeout_seconds)}ç§’ï¼‰...")
        start_wait_time = time.time()

        buffer = []
        offset = 0
        window_size = self.vad.config.silero_vad.window_size
        samples_per_read = int(0.1 * SAMPLE_RATE)

        started = False
        started_time = None

        with sd.InputStream(channels=1, dtype="float32", samplerate=SAMPLE_RATE) as audio_stream:
            while not killed:
                # ç©ºé—²è¶…æ—¶ï¼ˆå°šæœªå¼€å§‹è¯´è¯ï¼‰
                if not started and (time.time() - start_wait_time) > idle_timeout_seconds:
                    print("âŒ› è¶…æ—¶æœªæ£€æµ‹åˆ°è¯´è¯")
                    return "", True

                samples, _ = audio_stream.read(samples_per_read)
                samples = samples.reshape(-1)

                buffer = np.concatenate([buffer, samples])

                # VADå¤„ç†
                while offset + window_size < len(buffer):
                    self.vad.accept_waveform(buffer[offset : offset + window_size])
                    if not started and self.vad.is_speech_detected():
                        started = True
                        started_time = time.time()
                        print("   ğŸ¤ å¼€å§‹è¯´è¯...")
                    offset += window_size

                # æ¸…ç†ç¼“å†²åŒº
                if not started:
                    if len(buffer) > 10 * window_size:
                        offset -= len(buffer) - 10 * window_size
                        buffer = buffer[-10 * window_size :]

                # å®æ—¶è¯†åˆ«ï¼ˆä¸æ‰“å°ï¼Œåªç”¨äºå†…éƒ¨å¤„ç†ï¼‰
                if started and time.time() - started_time > 0.2:
                    stream = self.asr.create_stream()
                    stream.accept_waveform(SAMPLE_RATE, buffer)
                    self.asr.decode_stream(stream)
                    started_time = time.time()

                # å¤„ç†VADæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µï¼ˆæ£€æµ‹åˆ°é™éŸ³ç»“æŸï¼‰
                while not self.vad.empty():
                    stream = self.asr.create_stream()
                    stream.accept_waveform(SAMPLE_RATE, self.vad.front.samples)
                    self.vad.pop()
                    self.asr.decode_stream(stream)
                    text = stream.result.text.strip()

                    buffer = []
                    offset = 0
                    started = False
                    started_time = None

                    if text:
                        print(f"   âœ“ è¯†åˆ«å®Œæˆ: {text}")
                        return text, False
                    
                    # VADæ£€æµ‹åˆ°é™éŸ³æ®µç»“æŸï¼Œä½†æ²¡è¯†åˆ«åˆ°å†…å®¹ï¼Œé‡ç½®ç­‰å¾…æ—¶é—´
                    start_wait_time = time.time()
        
        return "", False
    
    def call_llm(self, user_input):
        """è°ƒç”¨å¤§æ¨¡å‹"""
        # æ„å»ºæç¤ºè¯
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        
        # RAG æ£€ç´¢å¢å¼ºï¼šå°†ç›¸å…³èµ„æ–™æ³¨å…¥åˆ°ç³»ç»Ÿä¸Šä¸‹æ–‡ä¸­
        rag_context = self.augment_with_rag(user_input)
        if rag_context:
            messages.append({
                "role": "system",
                "content": (
                    "ä»¥ä¸‹æ˜¯ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä½œç­”ï¼›è‹¥èµ„æ–™æœªæ¶µç›–ï¼Œå†ç»“åˆå¸¸è¯†ï¼š\n" 
                    + rag_context
                )
            })
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘3è½®ï¼‰
        for msg in self.conversation_history[-6:]:
            messages.append(msg)
        
        messages.append({"role": "user", "content": user_input})
        
        # æ„å»ºprompt
        prompt = ""
        for msg in messages:
            if msg["role"] == "system":
                prompt += f"System: {msg['content']}\n\n"
            elif msg["role"] == "user":
                prompt += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                prompt += f"Assistant: {msg['content']}\n"
        prompt += "Assistant: "
        
        print("ğŸ¤” å‚»å¦æ­£åœ¨æ€è€ƒ...")
        
        # è°ƒç”¨Ollama API
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            llm_response = result['response'].strip()
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "assistant", "content": llm_response})
            
            # åªä¿ç•™æœ€è¿‘5è½®
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]
            
            return llm_response
        else:
            return "æŠ±æ­‰ä¸»äººï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹ä¸èˆ’æœï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def run(self):
        """ä¸»å¾ªç¯"""
        global killed
        
        while not killed:
            # 1) å”¤é†’ä¸€æ¬¡
            if not self.wait_for_keyword():
                continue

            # 2) é—®å€™
            self.tts_speak("ä¸»äººï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ")

            # 3) è¿›å…¥å¯¹è¯å¾ªç¯ï¼šä¸å†éœ€è¦å†æ¬¡å”¤é†’
            while not killed:
                text, timed_out = self.listen_and_recognize_with_timeout(30)
                if timed_out:
                    # 30ç§’æ— äººè¯´è¯ï¼Œé€€ä¸‹
                    self.tts_speak("å‚»å¦é€€ä¸‹äº†ï¼Œéœ€è¦çš„æ—¶å€™å†å¬å”¤å‚»å¦å“¦ã€‚")
                    print("â€”â€” é€€å›å”¤é†’ç­‰å¾… â€”â€”")
                    break

                if not text:
                    print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹")
                    self.tts_speak("ä¸»äººï¼Œæˆ‘æ²¡å¬æ¸…ï¼Œè¯·å†è¯´ä¸€éã€‚")
                    continue

                # è°ƒç”¨LLM
                reply = self.call_llm(text)
                # æ’­æŠ¥
                self.tts_speak(reply)
                # å°åœé¡¿åç»§ç»­ç­‰å¾…ä¸‹ä¸€å¥
                time.sleep(0.3)
                print("\n" + "=" * 50 + "\n")


def main():
    print("\n" + "=" * 60)
    print(" " * 20 + "å‚»å¦è¯­éŸ³æ™ºèƒ½åŠ©æ‰‹")
    print("=" * 60)
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è„šæœ¬æ‰€åœ¨ç›®å½•: {SCRIPT_DIR}")
    
    # æ£€æŸ¥éº¦å…‹é£
    devices = sd.query_devices()
    if len(devices) == 0:
        print("âŒ æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡")
        sys.exit(-1)
    
    default_input_device_idx = sd.default.device[0]
    print(f"ä½¿ç”¨éº¦å…‹é£: {devices[default_input_device_idx]['name']}")
    
    # åˆ›å»ºå¹¶è¿è¡ŒåŠ©æ‰‹
    assistant = VoiceAssistant()
    assistant.initialize()
    
    try:
        assistant.run()
    except KeyboardInterrupt:
        print("\n\næ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"\nç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    print("å‚»å¦å·²é€€å‡ºï¼Œå†è§ï¼")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
